{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "\n",
    "def get_env():\n",
    "    '''\n",
    "    Arguments: \n",
    "            None\n",
    "    Returns:\n",
    "            states: array of states in the gridworld\n",
    "            goal_state: tuple containing the goal state\n",
    "            start_state: tuple containing the start state\n",
    "            wind: array containing the wind strengths of different columns\n",
    "            num_rows: number of rows in the gridworld\n",
    "            num_columns: number of columns in the gridworld\n",
    "    '''\n",
    "    num_rows = 7\n",
    "    num_columns = 10\n",
    "    goal_state = (3,7)\n",
    "    start_state = (3,0)\n",
    "    num_states = num_rows*num_columns\n",
    "    num_actions = 4\n",
    "    wind = [0,0,0,1,1,1,2,2,1,0]\n",
    "    states = []\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_columns):\n",
    "            states.append((i,j))\n",
    "            \n",
    "    return states,goal_state,start_state,wind,num_rows,num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the necessary move \n",
    "\n",
    "def make_move(current_position,move,goal_state,wind):\n",
    "    '''\n",
    "    Arguments: \n",
    "            current_position: tuple containing the current position of the agent\n",
    "            move: 'l','r','u' or 'd'\n",
    "            goal_state: tuple containing the final state\n",
    "            wind: array containing the wind strengths of different columns\n",
    "    Returns:\n",
    "            new_position: tuple containing the new position of the agent after the move is made\n",
    "            reward: reward assigned to the move made at the state depending on the next state reached\n",
    "    '''\n",
    "    \n",
    "    # Convert tuple to list so the operations are easier\n",
    "    current_position = list(current_position)\n",
    "    new_position = current_position\n",
    "    \n",
    "    # While making a move, it must be ensured that the agent doesn't go out of the environment\n",
    "    if move == 'l':\n",
    "        new_position[1] = max(0,current_position[1] - 1)\n",
    "    elif move == 'r':\n",
    "        new_position[1] = min(9,current_position[1] + 1)\n",
    "    elif move == 'u':\n",
    "        new_position[0] = max(0,current_position[0] - 1)\n",
    "    elif move == 'd':\n",
    "        new_position[0] = min(6,current_position[0] + 1)\n",
    "    \n",
    "    # Account for the wind in the corresponding column\n",
    "    new_position[0] -= wind[new_position[1]]\n",
    "    if new_position[0] < 0:\n",
    "        new_position[0] = 0\n",
    "    \n",
    "    # Reward is 1 if new position is the goal state and -1 otherwise\n",
    "    reward = 0\n",
    "    new_position = tuple(new_position)\n",
    "    if new_position == goal_state:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = -1\n",
    "    \n",
    "    return new_position,reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best action\n",
    "\n",
    "def best_action(Q,state):\n",
    "    '''\n",
    "    Arguments:\n",
    "            Q: The state-action value function\n",
    "            state: current state of the agent\n",
    "    Returns:\n",
    "            max_action: The optimal action in the state\n",
    "    '''\n",
    "    val = -1e17\n",
    "    max_action = 'l'\n",
    "    # Iterate through all state action-pairs for the state and find the best one\n",
    "    for action,value in Q[state].items():\n",
    "        if value > val:\n",
    "            val = value\n",
    "            max_action = action\n",
    "    \n",
    "    return max_action\n",
    "\n",
    "\n",
    "# Find an action using epsilon-greedy policy\n",
    "\n",
    "def get_action(Q,state,epsilon):\n",
    "    '''\n",
    "    Arguments:\n",
    "            Q: The state-action value function\n",
    "            state: current state of the agent\n",
    "    Returns:\n",
    "            action: action picked by the epsilon-greedy policy\n",
    "    '''\n",
    "    val = np.random.rand()\n",
    "    action = ''\n",
    "    # if val > epsilon, choose the best action, else choose a random action\n",
    "    if val > epsilon:\n",
    "        action = best_action(Q,state)\n",
    "    else:\n",
    "        list_actions = get_possible_actions(state)\n",
    "        action = random.choice(list_actions)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the list of possible action in a state\n",
    "\n",
    "def get_possible_actions(state):\n",
    "    '''\n",
    "    Arguments:\n",
    "            state: one of the states of the gridworld\n",
    "    Returns:\n",
    "            actions: list of possible actions from the given state\n",
    "    '''\n",
    "    actions = []\n",
    "    if state[0] > 0:\n",
    "        actions.append('u')\n",
    "    if state[0] < 6:\n",
    "        actions.append('d')\n",
    "    if state[1] > 0:\n",
    "        actions.append('l')\n",
    "    if state[1] < 9:\n",
    "        actions.append('r')\n",
    "    \n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of sarsa\n",
    "\n",
    "def sarsa(states,goal_state,start_state,wind,alpha = 0.1,epsilon = 0.1,gamma = 1,episodes=100):\n",
    "    '''\n",
    "    Arguments:\n",
    "            states: array of states in the gridworld\n",
    "            goal_state: tuple containing the goal state\n",
    "            start_state: tuple containing the start state\n",
    "            wind: array containing the wind strengths of different columns\n",
    "            alpha: step-size parameter\n",
    "            gamma: discount factor(1 here as the task is undiscounted)\n",
    "            episodes: number of episodes for training\n",
    "    Returns:\n",
    "            Q: state-action value function after sarsa is finished\n",
    "    '''\n",
    "    # Initialize the state-action value function\n",
    "    Q = {}\n",
    "    for state in states:\n",
    "        Q[state] = {}\n",
    "        possible_actions = get_possible_actions(state)\n",
    "        for action in possible_actions:\n",
    "            Q[state][action] = 0\n",
    "\n",
    "    # Iterate for required number of episodes\n",
    "    for episode in range(1,episodes+1):\n",
    "        # Initialize current state and action\n",
    "        current_state = start_state\n",
    "        current_action = get_action(Q,current_state,epsilon/episode)\n",
    "        \n",
    "        # Loop until goal is not reached\n",
    "        while current_state != goal_state:\n",
    "            \n",
    "            next_state,reward = make_move(current_state,current_action,goal_state,wind)\n",
    "            next_action = get_action(Q,next_state,epsilon/episode)       # Find next action based on epsilon-greedy policy\n",
    "\n",
    "            # Sarsa update\n",
    "            Q[current_state][current_action] = Q[current_state][current_action] + alpha*(reward + gamma*Q[next_state][next_action] - Q[current_state][current_action])\n",
    "            \n",
    "            current_state = next_state\n",
    "            current_action = next_action\n",
    "            if current_state == goal_state:\n",
    "                break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal policy \n",
    "\n",
    "def optimal_policy(Q,states,goal_state,start_state,wind,num_rows,num_columns):\n",
    "    '''\n",
    "    Arguments:\n",
    "            Q: state-action value function after sarsa is completed\n",
    "            states: array of states in the gridworld\n",
    "            goal_state: tuple containing the goal state\n",
    "            start_state: tuple containing the start state\n",
    "            wind: array containing the wind strengths of different columns\n",
    "            num_rows: number of rows in the gridworld\n",
    "            num_columns: number of columns in the gridworld\n",
    "    Returns:\n",
    "            visiting: array containing the steps taken in optimal path\n",
    "            returns: return when optimal path is followed\n",
    "            path: array containing the optimal path\n",
    "            \n",
    "    '''\n",
    "    current_state = start_state\n",
    "    path = []\n",
    "    path.append(current_state)\n",
    "    visiting = np.zeros((num_rows,num_columns))\n",
    "    steps = 1\n",
    "    returns = 0\n",
    "    # Follow greedy policy from start state until goal is reached\n",
    "    while 1:\n",
    "        action = best_action(Q,current_state)\n",
    "        next_state,r = make_move(current_state,action,goal_state,wind)\n",
    "        returns += r\n",
    "        visiting[next_state[0]][next_state[1]] = steps\n",
    "        current_state = next_state\n",
    "        steps += 1\n",
    "        path.append(current_state)\n",
    "        if current_state == goal_state:\n",
    "            break\n",
    "            \n",
    "    return visiting,returns,path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windy_gridworld():\n",
    "\n",
    "    # Define env\n",
    "    states,goal_state,start_state,wind,num_rows,num_columns = get_env()\n",
    "    \n",
    "    # Find state action value function\n",
    "    Q = sarsa(states,goal_state,start_state,wind,alpha = 0.5,epsilon = 0.1,gamma = 1,episodes = 1000)\n",
    "    \n",
    "    # Find optimal path\n",
    "    visiting,reward,path = optimal_policy(Q,states,goal_state,start_state,wind,num_rows,num_columns)\n",
    "    \n",
    "    print(\"The optimal path is:\")\n",
    "    path = '->'.join([str(item) for item in path])\n",
    "    print(path,'\\n')\n",
    "    print('The path taken can be visualized as:')\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_columns):\n",
    "            print('{:1.0f}'.format(visiting[i][j]), end=\"\\t\")\n",
    "        print()\n",
    "    print(f'\\nThe optimal return is {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal path is:\n",
      "(3, 0)->(3, 1)->(3, 2)->(2, 3)->(1, 4)->(0, 5)->(0, 6)->(0, 7)->(0, 8)->(0, 9)->(1, 9)->(2, 9)->(3, 9)->(4, 9)->(5, 9)->(6, 9)->(5, 8)->(3, 7) \n",
      "\n",
      "The path taken can be visualized as:\n",
      "0\t0\t0\t0\t0\t5\t6\t7\t8\t9\t\n",
      "0\t0\t0\t0\t4\t0\t0\t0\t0\t10\t\n",
      "0\t0\t0\t3\t0\t0\t0\t0\t0\t11\t\n",
      "0\t1\t2\t0\t0\t0\t0\t17\t0\t12\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t13\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t16\t14\t\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t15\t\n",
      "\n",
      "The optimal return is -15\n"
     ]
    }
   ],
   "source": [
    "windy_gridworld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
