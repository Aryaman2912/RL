{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialization():\n",
    "    '''\n",
    "    Returns:\n",
    "            V - a 2d array initialized as 0\n",
    "            R - array containing rewards for each state\n",
    "            P - array denoting equiprobable random policy\n",
    "    '''\n",
    "    V = np.zeros([4,4])\n",
    "    R = np.array([[0,-10,-10,-10],\n",
    "                  [-1,-1,-1,-1],\n",
    "                  [-10,-10,-10,-1],\n",
    "                  [-1,-1,-1,-1]]\n",
    "                )\n",
    "    # L - left, R - right, U - up, D - down\n",
    "    P = [[[],['L','R','D','U'],['L','R','D','U'],['L','D','R','U']],\n",
    "         [['U','R','D','L'],['U','R','L','D'],['U','R','L','D'],['U','L','D','R']],\n",
    "         [['U','R','D','L'],['U','R','L','D'],['U','R','L','D'],['U','L','D','R']],\n",
    "         [['U','R','L','D'],['L','R','U','D'],['L','R','U','D'],['L','U','R','D']]]\n",
    "    return V,R,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to implement Value iteration\n",
    "def value_iteration(V,P):\n",
    "    '''\n",
    "    Arguments:\n",
    "            V - 2d array initialized to 0\n",
    "            P - equiprobable random policy\n",
    "    Returns:\n",
    "            P - optimal policy\n",
    "    '''\n",
    "    num_iterations = 0\n",
    "    # Loop for convergence of value function\n",
    "    while num_iterations < 1000:\n",
    "        V1 = np.zeros([4,4])\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                # Evaluate value function using DP methods\n",
    "                if (i == 0 and j == 0):\n",
    "                    continue\n",
    "                elif i == 3 and j == 3:\n",
    "                    V1[i][j] = max(0.25*(R[i][j] + V[i][j-1]),0.25*(R[i][j] + V[i-1][j]))\n",
    "                elif i == 0 and j == 3:\n",
    "                    V1[i][j] = max(0.25*(R[i][j] + V[i][j-1]),0.25*(R[i][j] + V[i+1][j]))\n",
    "                elif i == 3 and j == 0:\n",
    "                    V1[i][j] = max(0.25*(R[i][j] + V[i-1][j]),0.25*(R[i][j] + V[i][j+1]))\n",
    "                elif i == 0:\n",
    "                    V1[i][j] = max(0.25*(R[i][j] + V[i][j-1]),0.25*(R[i][j] + V[i][j+1]),0.25*(R[i][j] + V[i+1][j]))\n",
    "                elif i == 3:\n",
    "                    V1[i][j] = max(0.25*(R[i][j] + V[i][j-1]),0.25*(R[i][j] + V[i][j+1]),0.25*(R[i][j] + V[i-1][j]))\n",
    "                elif j == 0:\n",
    "                    V1[i][j] = max(0.25*(R[i][j] + V[i][j+1]),0.25*(R[i][j] + V[i-1][j]),0.25*(R[i][j] + V[i+1][j]))\n",
    "                elif j == 3:\n",
    "                    V1[i][j] = max(0.25*(R[i][j] + V[i][j-1]),0.25*(R[i][j] + V[i-1][j]),0.25*(R[i][j] + V[i+1][j]))\n",
    "                else:\n",
    "                    V1[i][j] = max(0.25*(R[i][j] + V[i-1][j]),0.25*(R[i][j] + V[i+1][j]),0.25*(R[i][j] + V[i][j-1]),0.25*(R[i][j]+V[i][j+1]))\n",
    "        \n",
    "        V = V1\n",
    "        num_iterations += 1\n",
    "        \n",
    "    Actions = [[0,-1,'L'],[-1,0,'U'],[0,1,'R'],[1,0,'D']]\n",
    "    \n",
    "    # Find optimal policy using precomputed values of all states   \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if (i == 0 and j == 0):\n",
    "                continue\n",
    "            optimal_policy = []\n",
    "            max_val = -1e10\n",
    "            for a in Actions:\n",
    "                x = i + a[0]\n",
    "                y = j + a[1]\n",
    "                if x in range(4) and y in range(4): \n",
    "                    val = R[i][j] + V[x][y]\n",
    "                    if val > max_val:\n",
    "                        optimal_policy = [a[2]]\n",
    "                        max_val = val\n",
    "                    elif val == max_val:\n",
    "                        optimal_policy.append(a[2])\n",
    "            P[i][j] = optimal_policy\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal policy is: \n",
      "[[], ['L'], ['D'], ['D']]\n",
      "[['U'], ['L'], ['L'], ['L']]\n",
      "[['U'], ['U'], ['U'], ['U']]\n",
      "[['R'], ['R'], ['R'], ['U']]\n"
     ]
    }
   ],
   "source": [
    "V,R,P = initialization()\n",
    "P = value_iteration(V,P)\n",
    "# Print optimal policy\n",
    "print(\"The optimal policy is: \")\n",
    "for row in P:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
