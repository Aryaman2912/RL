{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization():\n",
    "    '''\n",
    "    Returns:\n",
    "            V - a 2d array initialized as 0\n",
    "            R - array containing rewards for each state\n",
    "            P - array denoting equiprobable random policy\n",
    "    '''\n",
    "    V = np.zeros([4,4])\n",
    "    R = np.array([[0,-10,-10,-10],\n",
    "                  [-1,-1,-1,-1],\n",
    "                  [-10,-10,-10,-1],\n",
    "                  [-1,-1,-1,-1]]\n",
    "                )\n",
    "    P = [[[],['L','R','D','U'],['L','R','D','U'],['L','D','R','U']],\n",
    "         [['U','R','D','L'],['U','R','L','D'],['U','R','L','D'],['U','L','D','R']],\n",
    "         [['U','R','D','L'],['U','R','L','D'],['U','R','L','D'],['U','L','D','R']],\n",
    "         [['U','R','L','D'],['L','R','U','D'],['L','R','U','D'],['L','U','R','D']]]\n",
    "    return V,R,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(V,R,P):\n",
    "    '''\n",
    "    Arguments:\n",
    "            V is a 2d array initialized as 0\n",
    "            R is the array containing rewards for each state\n",
    "            P is the policy taken by the agent\n",
    "    Returns:\n",
    "            V - The value function calculated for the policy P\n",
    "    '''\n",
    "    num_iterations = 0\n",
    "    while num_iterations < 1000:\n",
    "        V1 = np.zeros([4,4])\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if i == 0 and j == 0:\n",
    "                    continue\n",
    "                for a in P[i][j]:\n",
    "                    if a == 'L':\n",
    "                        if j-1 < 0:    \n",
    "                            V1[i][j] += 0.25*(R[i][j] + V[i][j])\n",
    "                        else:\n",
    "                            V1[i][j] += 0.25*(R[i][j] + V[i][j-1])\n",
    "                    elif a == 'R':\n",
    "                        if j + 1 >= 4:\n",
    "                            V1[i][j] += 0.25*(R[i][j] + V[i][j])\n",
    "                        else:\n",
    "                            V1[i][j] += 0.25*(R[i][j] + V[i][j+1])\n",
    "                    elif a == 'U':\n",
    "                        if i - 1 < 0:\n",
    "                            V1[i][j] += 0.25*(R[i][j] + V[i][j])\n",
    "                        else:\n",
    "                            V1[i][j] += 0.25*(R[i][j] + V[i-1][j])\n",
    "                    elif a == 'D':\n",
    "                        if i + 1 >= 4:\n",
    "                            V1[i][j] += 0.25*(R[i][j] + V[i][j])\n",
    "                        else:\n",
    "                            V1[i][j] += 0.25*(R[i][j] + V[i+1][j])\n",
    "        \n",
    "        V = V1\n",
    "        num_iterations += 1\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V,R,P,P1):\n",
    "    '''\n",
    "    Arguments: \n",
    "            V is the value function\n",
    "            R is the array containing rewards for each state\n",
    "            P is the equiprobable random policy\n",
    "            P1 is the previous optimal policy\n",
    "    Returns:\n",
    "            P - Optimal policy after performing policy improvement\n",
    "            policy_stable - bool variable denoting if P = P1\n",
    "    '''\n",
    "    policy_stable = True\n",
    "    # Iterate over all states to find the optimal policy\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            # Skip terminal state\n",
    "            if i == 0 and j == 0:\n",
    "                continue\n",
    "            optimal_policy = []\n",
    "            old_policy = P1[i][j]\n",
    "            max_val = -1e10\n",
    "            # Actions that take the agent out of the grid are skipped\n",
    "            for a in P[i][j]:\n",
    "                if a == 'L':\n",
    "                    if j - 1 < 0:\n",
    "                        continue\n",
    "                    val = R[i][j] + V[i][j-1]\n",
    "                    if val > max_val:\n",
    "                        max_val = val\n",
    "                        optimal_policy = ['L']\n",
    "                    elif val == max_val:\n",
    "                        optimal_policy.append('L')\n",
    "                elif a == 'R':\n",
    "                    if j + 1 >= 4:\n",
    "                        continue\n",
    "                    val = R[i][j] + V[i][j+1]\n",
    "                    if val > max_val:\n",
    "                        max_val = val\n",
    "                        optimal_policy = ['R']\n",
    "                    elif val == max_val:\n",
    "                        optimal_policy.append('R')\n",
    "                elif a == 'U':\n",
    "                    if i - 1 < 0:\n",
    "                        continue\n",
    "                    val = R[i][j] + V[i-1][j]\n",
    "                    if val > max_val:\n",
    "                        max_val = val\n",
    "                        optimal_policy = ['U']\n",
    "                    elif val == max_val:\n",
    "                        optimal_policy.append('U')\n",
    "                if a == 'D':\n",
    "                    if i + 1 >= 4:\n",
    "                        continue\n",
    "                    val = R[i][j] + V[i+1][j]\n",
    "                    if val > max_val:\n",
    "                        max_val = val\n",
    "                        optimal_policy = ['D']\n",
    "                    elif val == max_val:\n",
    "                        optimal_policy.append('D')\n",
    "                        \n",
    "            # Check if policy for state V[i][j] has changed\n",
    "            if old_policy != optimal_policy:\n",
    "                policy_stable = False\n",
    "                \n",
    "            P[i][j] = optimal_policy\n",
    "            \n",
    "    return P,policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], ['L'], ['D'], ['D']]\n",
      "[['U'], ['L'], ['L'], ['L']]\n",
      "[['U'], ['U'], ['U'], ['U']]\n",
      "[['R'], ['R'], ['R'], ['U']]\n"
     ]
    }
   ],
   "source": [
    "V,R,P1 = initialization()\n",
    "\n",
    "# Perform policy iteration until the policy doesn't change for any state in an iteration\n",
    "while True:\n",
    "    # Equiprobable random policy\n",
    "    P = ([[[],['L','R','D','U'],['L','R','D','U'],['L','D','R','U']],\n",
    "         [['U','R','D','L'],['U','R','L','D'],['U','R','L','D'],['U','L','D','R']],\n",
    "         [['U','R','D','L'],['U','R','L','D'],['U','R','L','D'],['U','L','D','R']],\n",
    "         [['U','R','L','D'],['L','R','U','D'],['L','R','U','D'],['L','U','R','D']]])\n",
    "\n",
    "    P1,policy_stable = policy_improvement(V,R,P,P1)\n",
    "    # If policy stable is true, the policy hasn't changed for any state in an iteration\n",
    "    if policy_stable:\n",
    "        break\n",
    "    V = policy_evaluation(V,R,P1)\n",
    "\n",
    "# Print optimal policy\n",
    "# Each cell denotes the optimal action that needs to be taken from that state\n",
    "for row in P1:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
